# ðŸ³ Kubernetes Production Issues

Welcome to the **Kubernetes Production Issues** repository!  
This repo documents **real-world Kubernetes production problems** and how they were diagnosed, fixed, and prevented. Itâ€™s meant as a **cheat sheet for DevOps engineers and K8s admins**.  

> ðŸ’¡ Pro Tip: Always test fixes in a staging cluster before applying them to production.

---

### ðŸ”¹ Scenario #1: Zombie Pods Blocking Node Drain
**Category:** Cluster Management  
**Environment:** K8s v1.23, On-prem bare metal, Systemd cgroups  

**Summary:** Node drain stuck indefinitely due to unresponsive pod with a finalizer.  

**What Happened:**  
- Pod had a **custom finalizer** that never completed termination.  
- API server kept waiting; pod stuck in **Terminating** state.

**Diagnosis Steps:**  
- `kubectl get pods --all-namespaces -o wide` â†’ found stuck pod.  
- `kubectl describe pod <pod>` â†’ saw custom finalizer.  
- Controller managing finalizer had **crashed**.

**Root Cause:** Finalizer logic never executed.  

**Fix / Workaround:**  
```bash
kubectl patch pod <pod-name> -p '{"metadata":{"finalizers":[]}}' --type=merge
```
---

### ðŸ”¹ Scenario #2: API Server Crash Due to Excessive CRD Writes
**Category:** Cluster Management  
**Environment:** K8s v1.24, GKE  

**Summary:** API server crashed because a malfunctioning controller flooded the cluster with custom resources.  

**What Happened:**  
- A bug in a controller caused it to create **thousands of Custom Resources (CRs)** in a tight reconciliation loop.  
- Etcd became overloaded â†’ slow writes â†’ API server eventually became unresponsive.  

**Diagnosis Steps:**  
- API latency increased â†’ frequent **504 Gateway Timeout** errors in `kubectl`.  
- `kubectl get crds | wc -l` â†’ huge number of CRs present.  
- Controller logs â†’ infinite reconcile loop on a specific CR type.  
- Etcd disk I/O maxed â†’ database operations slowed down.

**Root Cause:** Bad reconcile logic: the controller **always called create**, regardless of resource state, flooding the cluster with CRs.  

**Fix / Workaround:**  
- Scale the malfunctioning controller to 0 replicas.  
- Manually delete thousands of stale CRs in batches.

**Lessons Learned / Tips:**  
- Always test reconcile logic in a sandbox environment before deploying to production.  
- Implement **create/update guards** in reconciliation loops.  
- Add **Prometheus alerts** for unusually high CR counts.  

**Pro Tip:** Consider implementing **rate limiting** or **circuit breakers** in controllers to prevent uncontrolled resource creation.

---

### ðŸ”¹ Scenario #3: Node Not Rejoining After Reboot
**Category:** Cluster Management
**Environment:** K8s v1.21, Self-managed cluster, Static nodes

**Summary:** A rebooted node failed to rejoin the cluster due to kubelet identity mismatch.
**What Happened:** 
- After a kernel upgrade and reboot, a node didnâ€™t appear in kubectl get nodes. The kubelet logs showed registration issues.

**Diagnosis Steps:**
- Checked system logs and kubelet logs.
- Noticed --hostname-override didn't match the node name registered earlier.
- kubectl get nodes -o wide showed old hostname; new one mismatched due to DHCP/hostname change.

**Root Cause:** Kubelet registered with a hostname that no longer matched its node identity in the cluster.

**Fix / Workaround:**
- Re-joined the node using correct --hostname-override.
- Cleaned up stale node entry from the cluster.

**Lessons Learned:**
- Node identity must remain consistent across reboots.

**How to Avoid:**
- Set static hostnames and IPs.
- Use consistent cloud-init or kubeadm configuration.

---

### ðŸ”¹ Scenario #4: Etcd Disk Full Causing API Server Timeout
**Category:** Cluster Management
**Environment:** K8s v1.25, Bare-metal cluster

**Summary:** etcd ran out of disk space, making API server unresponsive.
**What Happened:**
- The cluster started failing API requests. Etcd logs showed disk space errors, and API server logs showed failed storage operations.

**Diagnosis Steps:**
- Used df -h on etcd nodes â€” confirmed disk full.
- Reviewed /var/lib/etcd â€“ excessive WAL and snapshot files.
- Used etcdctl to assess DB size.

**Root Cause:** Lack of compaction and snapshotting caused disk to fill up with historical revisions and WALs.

**Fix/Workaround:**
- bash
- CopyEdit
- etcdctl compact <rev>
- etcdctl defrag
  - Cleaned logs, snapshots, and increased disk space temporarily.

**Lessons Learned:**
- etcd requires periodic maintenance.

**How to Avoid:**
- Enable automatic compaction.
- Monitor disk space usage of etcd volumes.

---

### ðŸ”¹ Scenario #5: Misconfigured Taints Blocking Pod Scheduling
**Category:** Cluster Management
**Environment:** K8s v1.26, Multi-tenant cluster

**Summary:** Critical workloads werenâ€™t getting scheduled due to incorrect node taints.
**What Happened:**
A user added taints (NoSchedule) to all nodes to isolate their app, but forgot to include tolerations in workloads. Other apps stopped working.

**Diagnosis Steps:**
- Pods stuck in Pending state.
- Used kubectl describe pod <pod> â€“ reason: no nodes match tolerations.
- Inspected node taints via kubectl describe node.

**Root Cause:** Lack of required tolerations on most workloads.

**Fix/Workaround:**
- Removed the inappropriate taints.
- Re-scheduled workloads.

**Lessons Learned:**
- Node taints must be reviewed cluster-wide.

**How to Avoid:**
- Educate teams on node taints and tolerations.
- Restrict RBAC for node mutation.

---

### ðŸ”¹ Scenario #6: Kubelet DiskPressure Loop on Large Image Pulls
**Category:** Cluster Management
**Environment:** K8s v1.22, EKS

**Summary:** Continuous pod evictions caused by DiskPressure due to image bloating.
**What Happened:**
A new container image with many layers was deployed. Nodeâ€™s disk filled up, triggering kubeletâ€™s DiskPressure condition. Evicted pods created a loop.

**Diagnosis Steps:**
- Checked node conditions: kubectl describe node showed DiskPressure: True.
- Monitored image cache with crictl images.
- Node /var/lib/containerd usage exceeded threshold.

**Root Cause:** Excessive layering in container image and high pull churn caused disk exhaustion.
